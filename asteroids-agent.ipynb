{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals\n",
    "STATE_SHAPE = (210, 160, 3)\n",
    "ACTION_SIZE = 14\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_dqn(network, state, epsilon, epsilon_decay):\n",
    "    \"\"\"Select action according to e-greedy policy and decay epsilon\n",
    "\n",
    "    Args:\n",
    "        network (QNetwork): Q-Network\n",
    "        state (np-array): current state, size (state_size)\n",
    "        epsilon (float): probability of choosing a random action\n",
    "        epsilon_decay (float): amount by which to decay epsilon\n",
    "\n",
    "    Returns:\n",
    "        action (int): chosen action [0, action_size)\n",
    "        epsilon (float): decayed epsilon\n",
    "    \"\"\" \n",
    "    if random.uniform(0,1) > epsilon:\n",
    "        x = torch.FloatTensor(state).unsqueeze(0)\n",
    "        qs = network(x)\n",
    "        #print(qs.shape)\n",
    "        values, indices = qs.squeeze().max(0)\n",
    "        choice = indices.item()\n",
    "    else:\n",
    "        # in this case, choose randomly\n",
    "        choice = random.randint(0,ACTION_SIZE-1)\n",
    "\n",
    "    epsilon *= epsilon_decay\n",
    "    return choice, epsilon\n",
    "\n",
    "\n",
    "def prepare_batch(memory, batch_size):\n",
    "    \"\"\"Randomly sample batch from memory\n",
    "     Prepare cuda tensors\n",
    "\n",
    "    Args:\n",
    "        memory (list): state, action, next_state, reward, done tuples\n",
    "        batch_size (int): amount of memory to sample into a batch\n",
    "\n",
    "    Returns:\n",
    "        state (tensor): float cuda tensor of size (batch_size x state_size()\n",
    "        action (tensor): long tensor of size (batch_size)\n",
    "        next_state (tensor): float cuda tensor of size (batch_size x state_size)\n",
    "        reward (tensor): float cuda tensor of size (batch_size)\n",
    "        done (tensor): float cuda tensor of size (batch_size)\n",
    "    \"\"\"\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    state = torch.tensor([t[0] for t in batch])\n",
    "    action = torch.tensor([t[1] for t in batch])\n",
    "    next_state = torch.tensor([t[2] for t in batch])\n",
    "    reward = torch.tensor([t[3] for t in batch])\n",
    "    done = torch.tensor([t[4] for t in batch])\n",
    "\n",
    "    return state.float(), action.long(), next_state.float(), reward.float(), done.float() \n",
    "\n",
    "\n",
    "def learn_dqn(batch, optim, q_network, target_network, gamma, global_step, target_update):\n",
    "    \"\"\"Update Q-Network according to DQN Loss function\n",
    "     Update Target Network every target_update global steps\n",
    "\n",
    "    Args:\n",
    "        batch (tuple): tuple of state, action, next_state, reward, and done tensors\n",
    "        optim (Adam): Q-Network optimizer\n",
    "        q_network (QNetwork): Q-Network\n",
    "        target_network (QNetwork): Target Q-Network\n",
    "        gamma (float): discount factor\n",
    "        global_step (int): total steps taken in environment\n",
    "        target_update (int): frequency of target network update\n",
    "    \"\"\"\n",
    "    optim.zero_grad()\n",
    "\n",
    "    state, action, next_state, reward, done = batch\n",
    "\n",
    "    q_actions = q_network.forward(torch.FloatTensor(state))\n",
    "    target_actions = target_network.forward(torch.FloatTensor(next_state))\n",
    "\n",
    "    # Get the q-values for the chosen actions\n",
    "    q_sa_list = [q_actions[i][a] for i,a in enumerate(list(action))]\n",
    "    q_sa = q_sa_list[0].unsqueeze(0)\n",
    "    for i in range(1,BATCH_SIZE):\n",
    "        q_sa = torch.cat((q_sa,q_sa_list[i].unsqueeze(0)))\n",
    "\n",
    "    target_actions, _ = target_actions.max(1)\n",
    "    qstar_sa = target_actions\n",
    "    ans = (q_sa - (reward + gamma *qstar_sa*(1-done.int()))) ** 2\n",
    "\n",
    "    loss = torch.sum(ans)\n",
    "    loss.backward()\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    if global_step % target_update == 0:\n",
    "        target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Value Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        hidden_size = 8\n",
    "        c,h,w = state_size\n",
    "        self.net1 = nn.Sequential(nn.Conv2d(c,64,(3,3),padding=(1,1)),\n",
    "                                    nn.Conv1d(64,64,(3,3),padding=(1,1)),\n",
    "                                    nn.MaxPool2d((3, 3), stride=(2, 2), padding = (1,1), dilation = (1,1)),\n",
    "                                    nn.Conv2d(64,128,(3,3),padding=(1,1)),\n",
    "                                    nn.Conv2d(128,128,(3,3),padding=(1,1)),\n",
    "                                    nn.MaxPool2d((3, 3), stride=(2, 2), padding = (1,1), dilation = (1,1)))\n",
    "        \n",
    "        self.net2 = nn.Sequential(nn.Linear(5120, hidden_size),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(hidden_size, hidden_size),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(hidden_size, hidden_size),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(hidden_size, ACTION_SIZE))  \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Estimate q-values given state\n",
    "\n",
    "          Args:\n",
    "              state (tensor): current state, size (batch x state_size)\n",
    "\n",
    "          Returns:\n",
    "              q-values (tensor): estimated q-values, size (batch x action_size)\n",
    "        \"\"\"\n",
    "        a = self.net1(x)\n",
    "        b = a.view(-1, 128*40)\n",
    "        return self.net2(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_main():\n",
    "    # Hyper parameters\n",
    "    lr = 1e-3\n",
    "    epochs = 20\n",
    "    start_training = 1000\n",
    "    gamma = 0.99\n",
    "    batch_size = 32\n",
    "    epsilon = 1\n",
    "    epsilon_decay = .9999\n",
    "    target_update = 1000\n",
    "    learn_frequency = 3\n",
    "    max_reward = 60000\n",
    "\n",
    "    # Init environment\n",
    "    env = gym.make('Asteroids-v0', )\n",
    "\n",
    "    # Init networks\n",
    "    q_network = QNetwork(STATE_SHAPE, ACTION_SIZE)\n",
    "    target_network = QNetwork(STATE_SHAPE, ACTION_SIZE)\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    # Init optimizer\n",
    "    optim = torch.optim.Adam(q_network.parameters(), lr=lr)\n",
    "\n",
    "    # Init replay buffer\n",
    "    memory = []\n",
    "\n",
    "    # Begin main loop\n",
    "    results_dqn = []\n",
    "    global_step = 0\n",
    "    loop = tqdm(total=epochs, position=0, leave=False)\n",
    "    loss = 0\n",
    "    losses = []\n",
    "    \n",
    "    #stop = False\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        #if stop:\n",
    "            #break\n",
    "\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        cum_reward = 0  # Track cumulative reward per episode\n",
    "\n",
    "        # Begin episode\n",
    "        # TODO - I need to change this reward to match the reward of Asteroids\n",
    "        while not done and cum_reward < max_reward:  # End after max_reward is reached \n",
    "            # Select e-greedy action\n",
    "            action, epsilon = get_action_dqn(q_network, state, epsilon, epsilon_decay)\n",
    "\n",
    "            # Take step\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # env.render()\n",
    "\n",
    "            # Store step in replay buffer\n",
    "            memory.append((state, action, next_state, reward, done))\n",
    "\n",
    "            cum_reward += reward\n",
    "            global_step += 1  # Increment total steps\n",
    "            state = next_state  # Set current state\n",
    "\n",
    "            # If time to train\n",
    "            if global_step > start_training and global_step % learn_frequency == 0:\n",
    "\n",
    "                # Sample batch\n",
    "                batch = prepare_batch(memory, batch_size)\n",
    "\n",
    "                # Train\n",
    "                loss = learn_dqn(batch, optim, q_network, target_network, gamma, global_step, target_update)\n",
    "                \n",
    "                #stop = True\n",
    "                #break\n",
    "                \n",
    "        # Print results at end of episode\n",
    "        results_dqn.append(cum_reward)\n",
    "        loop.update(1)\n",
    "        loop.set_description('Episodes: {} Reward: {} Loss: {}'.format(epoch, cum_reward, loss))\n",
    "        losses.append(loss)\n",
    "\n",
    "    return results_dqn, losses, q_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afa25e6ad924802b3414d19284661e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f49c12138003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_dqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-d0ac02192331>\u001b[0m in \u001b[0;36mdqn_main\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;31m# Sample batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-50b633a3c8ce>\u001b[0m in \u001b[0;36mprepare_batch\u001b[0;34m(memory, batch_size)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_dqn, losses, q_network = dqn_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"models/q_net.pt\"\n",
    "torch.save(q_network,PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsteroidsAgent(object):\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        #return observation \n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Asteroids-v0')\n",
    "env.seed(0)\n",
    "agent = AsteroidsAgent(env.action_space)\n",
    "\n",
    "episode_count = 1\n",
    "reward = 0\n",
    "done = False\n",
    "\n",
    "for i in range(episode_count):\n",
    "    ob = env.reset()\n",
    "    while True:\n",
    "        action = agent.act(ob, reward, done)\n",
    "        env.render()\n",
    "        ob, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsteroidsModel(nn.Module):\n",
    "    def __init__(self,input_ex,out_ex):\n",
    "        super(AsteroidsModel, self).__init__()\n",
    "        \n",
    "        c,h,w = input_ex.size()\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = torch.zeros(14)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor(ob)\n",
    "#out = torch.zeros((agent.action_space.n))\n",
    "inp.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AsteroidsModel(inp,out)\n",
    "y_hat = model(inp)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c,h,w = inp.size()\n",
    "conv1 = nn.Conv2d(c,64,(3),padding=(1,1))\n",
    "a = conv1(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
