{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals\n",
    "ACTION_SIZE = 14\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_dqn(network, state, epsilon, epsilon_decay):\n",
    "    \"\"\"Select action according to e-greedy policy and decay epsilon\n",
    "\n",
    "    Args:\n",
    "        network (QNetwork): Q-Network\n",
    "        state (np-array): current state, size (state_size)\n",
    "        epsilon (float): probability of choosing a random action\n",
    "        epsilon_decay (float): amount by which to decay epsilon\n",
    "\n",
    "    Returns:\n",
    "        action (int): chosen action [0, action_size)\n",
    "        epsilon (float): decayed epsilon\n",
    "    \"\"\" \n",
    "    if random.uniform(0,1) > epsilon:\n",
    "        x = torch.cuda.FloatTensor(state)\n",
    "        qs = network(x)\n",
    "        values, indices = qs.max(0)\n",
    "        choice = indices.item()\n",
    "    else:\n",
    "        # in this case, choose randomly\n",
    "        choice = random.randint(0,ACTION_SIZE-1)\n",
    "\n",
    "    epsilon *= epsilon_decay\n",
    "    return choice, epsilon\n",
    "\n",
    "\n",
    "def prepare_batch(memory, batch_size):\n",
    "    \"\"\"Randomly sample batch from memory\n",
    "     Prepare cuda tensors\n",
    "\n",
    "    Args:\n",
    "        memory (list): state, action, next_state, reward, done tuples\n",
    "        batch_size (int): amount of memory to sample into a batch\n",
    "\n",
    "    Returns:\n",
    "        state (tensor): float cuda tensor of size (batch_size x state_size()\n",
    "        action (tensor): long tensor of size (batch_size)\n",
    "        next_state (tensor): float cuda tensor of size (batch_size x state_size)\n",
    "        reward (tensor): float cuda tensor of size (batch_size)\n",
    "        done (tensor): float cuda tensor of size (batch_size)\n",
    "    \"\"\"\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    state = torch.tensor([t[0] for t in batch])\n",
    "    action = torch.tensor([t[1] for t in batch])\n",
    "    next_state = torch.tensor([t[2] for t in batch])\n",
    "    reward = torch.tensor([t[3] for t in batch])\n",
    "    done = torch.tensor([t[4] for t in batch])\n",
    "\n",
    "    return state.float().cuda(), action.long().cuda(), next_state.float().cuda(), reward.float().cuda(), done.float().cuda() \n",
    "\n",
    "\n",
    "def learn_dqn(batch, optim, q_network, target_network, gamma, global_step, target_update):\n",
    "    \"\"\"Update Q-Network according to DQN Loss function\n",
    "     Update Target Network every target_update global steps\n",
    "\n",
    "    Args:\n",
    "        batch (tuple): tuple of state, action, next_state, reward, and done tensors\n",
    "        optim (Adam): Q-Network optimizer\n",
    "        q_network (QNetwork): Q-Network\n",
    "        target_network (QNetwork): Target Q-Network\n",
    "        gamma (float): discount factor\n",
    "        global_step (int): total steps taken in environment\n",
    "        target_update (int): frequency of target network update\n",
    "    \"\"\"\n",
    "    optim.zero_grad()\n",
    "\n",
    "    state, action, next_state, reward, done = batch\n",
    "\n",
    "    q_actions = q_network.forward(torch.cuda.FloatTensor(state))\n",
    "    target_actions = target_network.forward(torch.cuda.FloatTensor(next_state))\n",
    "\n",
    "    # Get the q-values for the chosen actions\n",
    "    q_sa_list = [q_actions[i][a] for i,a in enumerate(list(action))]\n",
    "    q_sa = q_sa_list[0].unsqueeze(0)\n",
    "    for i in range(1,BATCH_SIZE):\n",
    "        q_sa = torch.cat((q_sa,q_sa_list[i].unsqueeze(0)))\n",
    "\n",
    "    target_actions, _ = target_actions.max(1)\n",
    "    qstar_sa = target_actions\n",
    "    ans = (q_sa - (reward + gamma *qstar_sa*(1-done.int()))) ** 2\n",
    "\n",
    "    loss = torch.sum(ans)\n",
    "    loss.backward()\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    if global_step % target_update == 0:\n",
    "        target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Value Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        hidden_size = 8\n",
    "\n",
    "        self.net = nn.Sequential(nn.Linear(state_size, hidden_size),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(hidden_size, hidden_size),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(hidden_size, hidden_size),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(hidden_size, action_size))  \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Estimate q-values given state\n",
    "\n",
    "          Args:\n",
    "              state (tensor): current state, size (batch x state_size)\n",
    "\n",
    "          Returns:\n",
    "              q-values (tensor): estimated q-values, size (batch x action_size)\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "Fix the state size and action size so they reflect the state size and action size of asteroids!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_main():\n",
    "    # Hyper parameters\n",
    "    lr = 1e-3\n",
    "    epochs = 800\n",
    "    start_training = 1000\n",
    "    gamma = 0.99\n",
    "    batch_size = 32\n",
    "    epsilon = 1\n",
    "    epsilon_decay = .9999\n",
    "    target_update = 1000\n",
    "    learn_frequency = 2\n",
    "\n",
    "    # Init environment\n",
    "    state_size = 4\n",
    "    action_size = 2\n",
    "    env = gym.make('Asteroids-v0', )\n",
    "\n",
    "    # Init networks\n",
    "    q_network = QNetwork(state_size, action_size).cuda()\n",
    "    target_network = QNetwork(state_size, action_size).cuda()\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    # Init optimizer\n",
    "    optim = torch.optim.Adam(q_network.parameters(), lr=lr)\n",
    "\n",
    "    # Init replay buffer\n",
    "    memory = []\n",
    "\n",
    "    # Begin main loop\n",
    "    results_dqn = []\n",
    "    global_step = 0\n",
    "    loop = tqdm(total=epochs, position=0, leave=False)\n",
    "    loss = 0\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        cum_reward = 0  # Track cumulative reward per episode\n",
    "\n",
    "        # Begin episode\n",
    "        while not done and cum_reward < 200:  # End after 200 steps \n",
    "            # Select e-greedy action\n",
    "            action, epsilon = get_action_dqn(q_network, state, epsilon, epsilon_decay)\n",
    "\n",
    "            # Take step\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # env.render()\n",
    "\n",
    "            # Store step in replay buffer\n",
    "            memory.append((state, action, next_state, reward, done))\n",
    "\n",
    "            cum_reward += reward\n",
    "            global_step += 1  # Increment total steps\n",
    "            state = next_state  # Set current state\n",
    "\n",
    "            # If time to train\n",
    "            if global_step > start_training and global_step % learn_frequency == 0:\n",
    "\n",
    "                # Sample batch\n",
    "                batch = prepare_batch(memory, batch_size)\n",
    "\n",
    "                # Train\n",
    "                loss = learn_dqn(batch, optim, q_network, target_network, gamma, global_step, target_update)\n",
    "\n",
    "        # Print results at end of episode\n",
    "        results_dqn.append(cum_reward)\n",
    "        loop.update(1)\n",
    "        loop.set_description('Episodes: {} Reward: {} Loss: {}'.format(epoch, cum_reward, loss))\n",
    "        losses.append(loss)\n",
    "\n",
    "    return results_dqn, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dqn, losses = dqn_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsteroidsAgent(object):\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        \n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Asteroids-v0')\n",
    "env.seed(0)\n",
    "agent = AsteroidsAgent(env.action_space)\n",
    "\n",
    "episode_count = 1\n",
    "reward = 0\n",
    "done = False\n",
    "\n",
    "for i in range(episode_count):\n",
    "    ob = env.reset()\n",
    "    while True:\n",
    "        action = agent.act(ob, reward, done)\n",
    "        env.render()\n",
    "        ob, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsteroidsModel(nn.Module):\n",
    "    def __init__(self,input_ex,out_ex):\n",
    "        super(AsteroidsModel, self).__init__()\n",
    "        \n",
    "        c,h,w = input_ex.size()\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = torch.zeros(14)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([210, 160, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.tensor(ob)\n",
    "#out = torch.zeros((agent.action_space.n))\n",
    "inp.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AsteroidsModel(inp,out)\n",
    "y_hat = model(inp)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [64, 210, 3, 3], but got 3-dimensional input of size [210, 160, 3] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-14f02975b2cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    348\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    349\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 350\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [64, 210, 3, 3], but got 3-dimensional input of size [210, 160, 3] instead"
     ]
    }
   ],
   "source": [
    "c,h,w = inp.size()\n",
    "conv1 = nn.Conv2d(c,64,(3),padding=(1,1))\n",
    "a = conv1(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
